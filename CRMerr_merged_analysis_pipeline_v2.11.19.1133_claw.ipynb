{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Analysis Pipeline for CI148 Data\n",
    "## CRM, Vowel, and Consonant Perception Analysis\n",
    "\n",
    "This pipeline provides comprehensive analysis of:\n",
    "1. **CRM (Coordinate Response Measure)**: Speech-in-noise with gender-specific maskers\n",
    "2. **Vowel Perception**: 9-vowel identification in bimodal (BM) and CI conditions\n",
    "3. **Consonant Perception**: Consonant identification performance\n",
    "\n",
    "### Key Research Questions:\n",
    "- How does masker gender affect speech perception (VGRM)?\n",
    "- Are there differences between same vs. different gender masker conditions?\n",
    "- Do vowel and consonant perception patterns differ?\n",
    "- What are the relationships between different perceptual measures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind, ttest_rel, f_oneway, mannwhitneyu, wilcoxon, kruskal\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import glob\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for publication-quality figures\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('paper', font_scale=1.2)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['savefig.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_crm_file(filepath):\n",
    "    \"\"\"Parse CRM data file with automatic gender detection.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Extract header information\n",
    "    header = lines[0]\n",
    "    talker = int(re.search(r'Talker (\\d+)', header).group(1))\n",
    "    maskers_match = re.search(r'Maskers (\\d+) and (\\d+)', header)\n",
    "    masker1 = int(maskers_match.group(1))\n",
    "    masker2 = int(maskers_match.group(2))\n",
    "    \n",
    "    # Determine genders (0-3 = male, 4-7 = female)\n",
    "    target_gender = 'F' if talker >= 4 else 'M'\n",
    "    masker1_gender = 'F' if masker1 >= 4 else 'M'\n",
    "    masker2_gender = 'F' if masker2 >= 4 else 'M'\n",
    "    masker_genders = ''.join(sorted([masker1_gender, masker2_gender]))\n",
    "    \n",
    "    # Classify condition\n",
    "    if target_gender == 'M' and masker_genders == 'MM':\n",
    "        condition = 'M-MM'  # Same gender\n",
    "    elif target_gender == 'F' and masker_genders == 'FF':\n",
    "        condition = 'F-FF'  # Same gender\n",
    "    elif target_gender == 'M' and masker_genders == 'FF':\n",
    "        condition = 'M-FF'  # Different gender\n",
    "    elif target_gender == 'F' and masker_genders == 'MM':\n",
    "        condition = 'F-MM'  # Different gender\n",
    "    else:\n",
    "        condition = f'{target_gender}-{masker_genders}'  # Mixed maskers\n",
    "    \n",
    "    # Parse data\n",
    "    data = []\n",
    "    for line in lines[2:]:\n",
    "        if 'SRT' in line:\n",
    "            srt_match = re.search(r'SRT.*?([-\\d.]+) dB.*?SD.*?([-\\d.]+) dB', line)\n",
    "            if srt_match:\n",
    "                srt = float(srt_match.group(1))\n",
    "                sd = float(srt_match.group(2))\n",
    "                return {\n",
    "                    'file': filepath.name,\n",
    "                    'target': talker,\n",
    "                    'masker1': masker1,\n",
    "                    'masker2': masker2,\n",
    "                    'target_gender': target_gender,\n",
    "                    'masker_genders': masker_genders,\n",
    "                    'condition': condition,\n",
    "                    'gender_match': 'same' if condition in ['M-MM', 'F-FF'] else 'different',\n",
    "                    'srt': srt,\n",
    "                    'sd': sd\n",
    "                }\n",
    "        elif line.strip() and not line.startswith('Run'):\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 6:\n",
    "                try:\n",
    "                    run = int(parts[0])\n",
    "                    col_target = int(parts[1])\n",
    "                    col_response = int(parts[2])\n",
    "                    num_target = int(parts[3])\n",
    "                    num_response = int(parts[4])\n",
    "                    snr = float(parts[5])\n",
    "                    correct = (col_target == col_response) and (num_target == num_response)\n",
    "                    data.append({\n",
    "                        'run': run,\n",
    "                        'col_target': col_target,\n",
    "                        'col_response': col_response,\n",
    "                        'num_target': num_target,\n",
    "                        'num_response': num_response,\n",
    "                        'snr': snr,\n",
    "                        'correct': correct\n",
    "                    })\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    return None\n",
    "\n",
    "def parse_vowel_file(filepath):\n",
    "    \"\"\"Parse vowel identification data.\"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 5:\n",
    "                trial = int(parts[0])\n",
    "                target = int(parts[1])\n",
    "                response = int(parts[2])\n",
    "                correct = int(parts[3])\n",
    "                rt = float(parts[4])\n",
    "                data.append({\n",
    "                    'trial': trial,\n",
    "                    'target': target,\n",
    "                    'response': response,\n",
    "                    'correct': correct,\n",
    "                    'rt': rt\n",
    "                })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def parse_consonant_file(filepath):\n",
    "    \"\"\"Parse consonant identification data.\"\"\"\n",
    "    # This will depend on the specific format of consonant files\n",
    "    # Placeholder for now\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Process All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CI148_vow9_BM_0.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m crm_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(crm_data)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Load vowel data\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m vowel_bm \u001b[38;5;241m=\u001b[39m parse_vowel_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCI148_vow9_BM_0.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m vowel_ci \u001b[38;5;241m=\u001b[39m parse_vowel_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCI148_vow9_CI_0.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m vowel_bm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBimodal\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 78\u001b[0m, in \u001b[0;36mparse_vowel_file\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse vowel identification data.\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     80\u001b[0m         parts \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CI148_vow9_BM_0.txt'"
     ]
    }
   ],
   "source": [
    "# Load CRM data\n",
    "crm_files = glob.glob('CI148_crm_*.txt')\n",
    "crm_data = []\n",
    "\n",
    "for filepath in crm_files:\n",
    "    result = parse_crm_file(Path(filepath))\n",
    "    if result:\n",
    "        crm_data.append(result)\n",
    "\n",
    "crm_df = pd.DataFrame(crm_data)\n",
    "\n",
    "# Load vowel data\n",
    "vowel_bm = parse_vowel_file('CI148_vow9_BM_0.txt')\n",
    "vowel_ci = parse_vowel_file('CI148_vow9_CI_0.txt')\n",
    "vowel_bm['condition'] = 'Bimodal'\n",
    "vowel_ci['condition'] = 'CI'\n",
    "vowel_df = pd.concat([vowel_bm, vowel_ci], ignore_index=True)\n",
    "\n",
    "print(\"Data Loading Summary:\")\n",
    "print(f\"CRM files loaded: {len(crm_df)}\")\n",
    "print(f\"Vowel trials: {len(vowel_df)} ({len(vowel_bm)} BM, {len(vowel_ci)} CI)\")\n",
    "print(\"\\nCRM Conditions:\")\n",
    "print(crm_df['condition'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Primary Analysis: Voice-Gender Release from Masking (VGRM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VGRM for each target gender\n",
    "def calculate_vgrm(df):\n",
    "    \"\"\"Calculate Voice-Gender Release from Masking.\"\"\"\n",
    "    vgrm_results = {}\n",
    "    \n",
    "    # For male targets\n",
    "    m_same = df[df['condition'] == 'M-MM']['srt'].values\n",
    "    m_diff = df[df['condition'] == 'M-FF']['srt'].values\n",
    "    if len(m_same) > 0 and len(m_diff) > 0:\n",
    "        vgrm_results['Male_Target'] = {\n",
    "            'same_gender_srt': m_same.mean(),\n",
    "            'diff_gender_srt': m_diff.mean(),\n",
    "            'vgrm': m_same.mean() - m_diff.mean(),\n",
    "            'n_same': len(m_same),\n",
    "            'n_diff': len(m_diff)\n",
    "        }\n",
    "    \n",
    "    # For female targets\n",
    "    f_same = df[df['condition'] == 'F-FF']['srt'].values\n",
    "    f_diff = df[df['condition'] == 'F-MM']['srt'].values\n",
    "    if len(f_same) > 0 and len(f_diff) > 0:\n",
    "        vgrm_results['Female_Target'] = {\n",
    "            'same_gender_srt': f_same.mean(),\n",
    "            'diff_gender_srt': f_diff.mean(),\n",
    "            'vgrm': f_same.mean() - f_diff.mean(),\n",
    "            'n_same': len(f_same),\n",
    "            'n_diff': len(f_diff)\n",
    "        }\n",
    "    \n",
    "    # Overall VGRM\n",
    "    same = df[df['gender_match'] == 'same']['srt'].values\n",
    "    diff = df[df['gender_match'] == 'different']['srt'].values\n",
    "    if len(same) > 0 and len(diff) > 0:\n",
    "        vgrm_results['Overall'] = {\n",
    "            'same_gender_srt': same.mean(),\n",
    "            'diff_gender_srt': diff.mean(),\n",
    "            'vgrm': same.mean() - diff.mean(),\n",
    "            'n_same': len(same),\n",
    "            'n_diff': len(diff)\n",
    "        }\n",
    "    \n",
    "    return vgrm_results\n",
    "\n",
    "vgrm_results = calculate_vgrm(crm_df)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VOICE-GENDER RELEASE FROM MASKING (VGRM) ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "for target, results in vgrm_results.items():\n",
    "    print(f\"\\n{target}:\")\n",
    "    print(f\"  Same-gender SRT: {results['same_gender_srt']:.2f} dB (n={results['n_same']})\")\n",
    "    print(f\"  Diff-gender SRT: {results['diff_gender_srt']:.2f} dB (n={results['n_diff']})\")\n",
    "    print(f\"  VGRM: {results['vgrm']:.2f} dB\")\n",
    "    print(f\"  {'BENEFIT' if results['vgrm'] > 0 else 'NO BENEFIT'} from different-gender maskers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Testing for VGRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests for VGRM\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL TESTING FOR VGRM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test overall gender match effect\n",
    "same_srt = crm_df[crm_df['gender_match'] == 'same']['srt']\n",
    "diff_srt = crm_df[crm_df['gender_match'] == 'different']['srt']\n",
    "\n",
    "if len(same_srt) > 0 and len(diff_srt) > 0:\n",
    "    # Parametric test\n",
    "    t_stat, t_pval = ttest_ind(same_srt, diff_srt)\n",
    "    # Non-parametric test\n",
    "    u_stat, u_pval = mannwhitneyu(same_srt, diff_srt, alternative='two-sided')\n",
    "    \n",
    "    print(\"\\nOverall Same vs Different Gender Maskers:\")\n",
    "    print(f\"  Independent t-test: t={t_stat:.3f}, p={t_pval:.4f}\")\n",
    "    print(f\"  Mann-Whitney U: U={u_stat:.1f}, p={u_pval:.4f}\")\n",
    "    print(f\"  Effect size (Cohen's d): {(same_srt.mean() - diff_srt.mean()) / np.sqrt((same_srt.var() + diff_srt.var()) / 2):.3f}\")\n",
    "\n",
    "# Test for specific contrasts\n",
    "contrasts = [\n",
    "    ('M-MM', 'M-FF', 'Male Target VGRM'),\n",
    "    ('F-FF', 'F-MM', 'Female Target VGRM'),\n",
    "    ('M-MM', 'F-FF', 'Same-Gender: Male vs Female'),\n",
    "    ('M-FF', 'F-MM', 'Different-Gender: Male vs Female')\n",
    "]\n",
    "\n",
    "for cond1, cond2, label in contrasts:\n",
    "    data1 = crm_df[crm_df['condition'] == cond1]['srt']\n",
    "    data2 = crm_df[crm_df['condition'] == cond2]['srt']\n",
    "    \n",
    "    if len(data1) > 0 and len(data2) > 0:\n",
    "        t_stat, t_pval = ttest_ind(data1, data2)\n",
    "        print(f\"\\n{label}:\")\n",
    "        print(f\"  {cond1}: {data1.mean():.2f} ± {data1.std():.2f} dB\")\n",
    "        print(f\"  {cond2}: {data2.mean():.2f} ± {data2.std():.2f} dB\")\n",
    "        print(f\"  Difference: {data1.mean() - data2.mean():.2f} dB\")\n",
    "        print(f\"  t-test: t={t_stat:.3f}, p={t_pval:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Visualization Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. SRT by condition\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "condition_order = ['M-MM', 'M-FF', 'F-FF', 'F-MM']\n",
    "sns.boxplot(data=crm_df, x='condition', y='srt', order=condition_order, ax=ax1)\n",
    "ax1.set_title('SRT by Target-Masker Configuration')\n",
    "ax1.set_ylabel('SRT (dB)')\n",
    "ax1.set_xlabel('Condition')\n",
    "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 2. VGRM comparison\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "vgrm_data = pd.DataFrame([\n",
    "    {'Target': 'Male', 'Masker': 'Same', 'SRT': crm_df[crm_df['condition'] == 'M-MM']['srt'].mean()},\n",
    "    {'Target': 'Male', 'Masker': 'Different', 'SRT': crm_df[crm_df['condition'] == 'M-FF']['srt'].mean()},\n",
    "    {'Target': 'Female', 'Masker': 'Same', 'SRT': crm_df[crm_df['condition'] == 'F-FF']['srt'].mean()},\n",
    "    {'Target': 'Female', 'Masker': 'Different', 'SRT': crm_df[crm_df['condition'] == 'F-MM']['srt'].mean()}\n",
    "])\n",
    "sns.barplot(data=vgrm_data, x='Target', y='SRT', hue='Masker', ax=ax2)\n",
    "ax2.set_title('VGRM by Target Gender')\n",
    "ax2.set_ylabel('SRT (dB)')\n",
    "ax2.legend(title='Masker Gender')\n",
    "\n",
    "# 3. Variability analysis\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "sns.barplot(data=crm_df, x='condition', y='sd', order=condition_order, ax=ax3)\n",
    "ax3.set_title('Response Variability by Condition')\n",
    "ax3.set_ylabel('Standard Deviation (dB)')\n",
    "ax3.set_xlabel('Condition')\n",
    "\n",
    "# 4. Vowel accuracy comparison\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "vowel_acc = vowel_df.groupby(['condition', 'target'])['correct'].mean().reset_index()\n",
    "sns.barplot(data=vowel_acc, x='target', y='correct', hue='condition', ax=ax4)\n",
    "ax4.set_title('Vowel Identification by Target')\n",
    "ax4.set_ylabel('Proportion Correct')\n",
    "ax4.set_xlabel('Target Vowel')\n",
    "ax4.legend(title='Condition')\n",
    "\n",
    "# 5. Confusion matrix for vowels (Bimodal)\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "confusion_bm = pd.crosstab(vowel_bm['target'], vowel_bm['response'], normalize='index')\n",
    "sns.heatmap(confusion_bm, annot=True, fmt='.2f', cmap='YlOrRd', ax=ax5, cbar_kws={'label': 'Proportion'})\n",
    "ax5.set_title('Vowel Confusion Matrix (Bimodal)')\n",
    "ax5.set_xlabel('Response')\n",
    "ax5.set_ylabel('Target')\n",
    "\n",
    "# 6. Confusion matrix for vowels (CI)\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "confusion_ci = pd.crosstab(vowel_ci['target'], vowel_ci['response'], normalize='index')\n",
    "sns.heatmap(confusion_ci, annot=True, fmt='.2f', cmap='YlOrRd', ax=ax6, cbar_kws={'label': 'Proportion'})\n",
    "ax6.set_title('Vowel Confusion Matrix (CI)')\n",
    "ax6.set_xlabel('Response')\n",
    "ax6.set_ylabel('Target')\n",
    "\n",
    "# 7. Reaction time analysis\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "sns.violinplot(data=vowel_df, x='condition', y='rt', ax=ax7)\n",
    "ax7.set_title('Reaction Time Distribution')\n",
    "ax7.set_ylabel('Reaction Time (s)')\n",
    "ax7.set_xlabel('Condition')\n",
    "\n",
    "# 8. Correlation matrix\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "crm_pivot = crm_df.pivot_table(values='srt', index='file', columns='condition')\n",
    "if len(crm_pivot.columns) > 1:\n",
    "    corr = crm_pivot.corr()\n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax8)\n",
    "    ax8.set_title('SRT Correlation Between Conditions')\n",
    "\n",
    "# 9. Overall performance summary\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "summary_data = pd.DataFrame({\n",
    "    'Measure': ['CRM Same', 'CRM Diff', 'Vowel BM', 'Vowel CI'],\n",
    "    'Performance': [\n",
    "        -crm_df[crm_df['gender_match'] == 'same']['srt'].mean(),\n",
    "        -crm_df[crm_df['gender_match'] == 'different']['srt'].mean(),\n",
    "        vowel_bm['correct'].mean() * 100,\n",
    "        vowel_ci['correct'].mean() * 100\n",
    "    ],\n",
    "    'Type': ['CRM', 'CRM', 'Vowel', 'Vowel']\n",
    "})\n",
    "sns.barplot(data=summary_data, x='Measure', y='Performance', hue='Type', ax=ax9)\n",
    "ax9.set_title('Overall Performance Summary')\n",
    "ax9.set_ylabel('Performance (Higher is Better)')\n",
    "ax9.legend().remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vowel Perception Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VOWEL PERCEPTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall accuracy\n",
    "print(\"\\nOverall Accuracy:\")\n",
    "print(f\"  Bimodal: {vowel_bm['correct'].mean():.3f} ({vowel_bm['correct'].sum()}/{len(vowel_bm)})\")\n",
    "print(f\"  CI: {vowel_ci['correct'].mean():.3f} ({vowel_ci['correct'].sum()}/{len(vowel_ci)})\")\n",
    "\n",
    "# Statistical comparison\n",
    "chi2_stat = stats.chi2_contingency(pd.crosstab(vowel_df['condition'], vowel_df['correct']))\n",
    "print(f\"\\nChi-square test: χ²={chi2_stat[0]:.3f}, p={chi2_stat[1]:.4f}\")\n",
    "\n",
    "# Per-vowel analysis\n",
    "print(\"\\nPer-Vowel Accuracy:\")\n",
    "vowel_summary = vowel_df.groupby(['condition', 'target'])['correct'].agg(['mean', 'count'])\n",
    "vowel_summary['CI_Benefit'] = vowel_summary.loc['CI', 'mean'] - vowel_summary.loc['Bimodal', 'mean']\n",
    "print(vowel_summary.round(3))\n",
    "\n",
    "# Identify most confused pairs\n",
    "print(\"\\nMost Confused Vowel Pairs (Bimodal):\")\n",
    "confusion_pairs_bm = []\n",
    "for target in range(1, 10):\n",
    "    target_data = vowel_bm[vowel_bm['target'] == target]\n",
    "    incorrect = target_data[target_data['correct'] == 0]\n",
    "    if len(incorrect) > 0:\n",
    "        most_common_error = incorrect['response'].value_counts().index[0]\n",
    "        error_rate = incorrect['response'].value_counts().values[0] / len(target_data)\n",
    "        confusion_pairs_bm.append((target, most_common_error, error_rate))\n",
    "\n",
    "confusion_pairs_bm.sort(key=lambda x: x[2], reverse=True)\n",
    "for target, response, rate in confusion_pairs_bm[:5]:\n",
    "    print(f\"  Vowel {target} → {response}: {rate:.3f}\")\n",
    "\n",
    "print(\"\\nMost Confused Vowel Pairs (CI):\")\n",
    "confusion_pairs_ci = []\n",
    "for target in range(1, 10):\n",
    "    target_data = vowel_ci[vowel_ci['target'] == target]\n",
    "    incorrect = target_data[target_data['correct'] == 0]\n",
    "    if len(incorrect) > 0:\n",
    "        most_common_error = incorrect['response'].value_counts().index[0]\n",
    "        error_rate = incorrect['response'].value_counts().values[0] / len(target_data)\n",
    "        confusion_pairs_ci.append((target, most_common_error, error_rate))\n",
    "\n",
    "confusion_pairs_ci.sort(key=lambda x: x[2], reverse=True)\n",
    "for target, response, rate in confusion_pairs_ci[:5]:\n",
    "    print(f\"  Vowel {target} → {response}: {rate:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reaction Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REACTION TIME ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall RT statistics\n",
    "print(\"\\nOverall Reaction Times:\")\n",
    "print(f\"  Bimodal: {vowel_bm['rt'].mean():.3f} ± {vowel_bm['rt'].std():.3f} s\")\n",
    "print(f\"  CI: {vowel_ci['rt'].mean():.3f} ± {vowel_ci['rt'].std():.3f} s\")\n",
    "\n",
    "t_stat, p_val = ttest_ind(vowel_bm['rt'], vowel_ci['rt'])\n",
    "print(f\"  t-test: t={t_stat:.3f}, p={p_val:.4f}\")\n",
    "\n",
    "# RT by accuracy\n",
    "print(\"\\nRT by Accuracy:\")\n",
    "for condition in ['Bimodal', 'CI']:\n",
    "    data = vowel_df[vowel_df['condition'] == condition]\n",
    "    correct_rt = data[data['correct'] == 1]['rt'].mean()\n",
    "    incorrect_rt = data[data['correct'] == 0]['rt'].mean()\n",
    "    print(f\"  {condition}:\")\n",
    "    print(f\"    Correct: {correct_rt:.3f} s\")\n",
    "    print(f\"    Incorrect: {incorrect_rt:.3f} s\")\n",
    "    print(f\"    Difference: {incorrect_rt - correct_rt:.3f} s\")\n",
    "\n",
    "# RT correlations\n",
    "print(\"\\nRT-Accuracy Correlations:\")\n",
    "for condition in ['Bimodal', 'CI']:\n",
    "    data = vowel_df[vowel_df['condition'] == condition]\n",
    "    per_vowel = data.groupby('target').agg({'rt': 'mean', 'correct': 'mean'})\n",
    "    if len(per_vowel) > 2:\n",
    "        r, p = pearsonr(per_vowel['rt'], per_vowel['correct'])\n",
    "        print(f\"  {condition}: r={r:.3f}, p={p:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Exploratory Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADVANCED EXPLORATORY ANALYSES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Learning/Adaptation Effects\n",
    "print(\"\\n1. LEARNING/ADAPTATION EFFECTS:\")\n",
    "# Split vowel trials into blocks\n",
    "n_blocks = 4\n",
    "for condition_name, condition_data in [('Bimodal', vowel_bm), ('CI', vowel_ci)]:\n",
    "    block_size = len(condition_data) // n_blocks\n",
    "    condition_data['block'] = condition_data.index // block_size + 1\n",
    "    block_acc = condition_data.groupby('block')['correct'].mean()\n",
    "    print(f\"\\n{condition_name} - Accuracy by block:\")\n",
    "    for block, acc in block_acc.items():\n",
    "        if block <= n_blocks:\n",
    "            print(f\"  Block {block}: {acc:.3f}\")\n",
    "    \n",
    "    # Test for linear trend\n",
    "    if len(block_acc) > 1:\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(range(1, len(block_acc)+1), block_acc.values)\n",
    "        print(f\"  Linear trend: slope={slope:.4f}, r={r_value:.3f}, p={p_value:.4f}\")\n",
    "\n",
    "# 2. Response Bias Analysis\n",
    "print(\"\\n2. RESPONSE BIAS ANALYSIS:\")\n",
    "for condition_name, condition_data in [('Bimodal', vowel_bm), ('CI', vowel_ci)]:\n",
    "    response_freq = condition_data['response'].value_counts()\n",
    "    target_freq = condition_data['target'].value_counts()\n",
    "    bias = response_freq - target_freq\n",
    "    print(f\"\\n{condition_name} - Response bias (response freq - target freq):\")\n",
    "    print(bias.sort_values(ascending=False).head().to_string())\n",
    "\n",
    "# 3. Speed-Accuracy Trade-off\n",
    "print(\"\\n3. SPEED-ACCURACY TRADE-OFF:\")\n",
    "for condition_name, condition_data in [('Bimodal', vowel_bm), ('CI', vowel_ci)]:\n",
    "    # Median split on RT\n",
    "    median_rt = condition_data['rt'].median()\n",
    "    fast_trials = condition_data[condition_data['rt'] < median_rt]\n",
    "    slow_trials = condition_data[condition_data['rt'] >= median_rt]\n",
    "    print(f\"\\n{condition_name}:\")\n",
    "    print(f\"  Fast trials: {fast_trials['correct'].mean():.3f} accuracy, {fast_trials['rt'].mean():.3f}s RT\")\n",
    "    print(f\"  Slow trials: {slow_trials['correct'].mean():.3f} accuracy, {slow_trials['rt'].mean():.3f}s RT\")\n",
    "    print(f\"  Accuracy difference: {slow_trials['correct'].mean() - fast_trials['correct'].mean():.3f}\")\n",
    "\n",
    "# 4. Spectral Distance Analysis (simplified vowel space)\n",
    "print(\"\\n4. SPECTRAL DISTANCE EFFECTS:\")\n",
    "# Define approximate vowel positions (simplified 2D space)\n",
    "vowel_positions = {\n",
    "    1: (0, 0), 2: (1, 0), 3: (2, 0),  # Front vowels\n",
    "    4: (0, 1), 5: (1, 1), 6: (2, 1),  # Central vowels\n",
    "    7: (0, 2), 8: (1, 2), 9: (2, 2)   # Back vowels\n",
    "}\n",
    "\n",
    "for condition_name, condition_data in [('Bimodal', vowel_bm), ('CI', vowel_ci)]:\n",
    "    errors = condition_data[condition_data['correct'] == 0]\n",
    "    if len(errors) > 0:\n",
    "        distances = []\n",
    "        for _, row in errors.iterrows():\n",
    "            if row['target'] in vowel_positions and row['response'] in vowel_positions:\n",
    "                pos1 = vowel_positions[row['target']]\n",
    "                pos2 = vowel_positions[row['response']]\n",
    "                dist = np.sqrt((pos1[0]-pos2[0])**2 + (pos1[1]-pos2[1])**2)\n",
    "                distances.append(dist)\n",
    "        if distances:\n",
    "            print(f\"\\n{condition_name} - Error distances in vowel space:\")\n",
    "            print(f\"  Mean distance: {np.mean(distances):.3f}\")\n",
    "            print(f\"  Median distance: {np.median(distances):.3f}\")\n",
    "\n",
    "# 5. Information Transfer Analysis\n",
    "print(\"\\n5. INFORMATION TRANSFER:\")\n",
    "from scipy.stats import entropy\n",
    "\n",
    "for condition_name, condition_data in [('Bimodal', vowel_bm), ('CI', vowel_ci)]:\n",
    "    # Calculate mutual information\n",
    "    confusion = pd.crosstab(condition_data['target'], condition_data['response'], normalize='index')\n",
    "    \n",
    "    # Marginal probabilities\n",
    "    p_target = condition_data['target'].value_counts(normalize=True)\n",
    "    p_response = condition_data['response'].value_counts(normalize=True)\n",
    "    \n",
    "    # Information measures\n",
    "    h_target = entropy(p_target)\n",
    "    h_response = entropy(p_response)\n",
    "    \n",
    "    # Percent information transmitted\n",
    "    percent_correct = condition_data['correct'].mean()\n",
    "    print(f\"\\n{condition_name}:\")\n",
    "    print(f\"  Target entropy: {h_target:.3f} bits\")\n",
    "    print(f\"  Response entropy: {h_response:.3f} bits\")\n",
    "    print(f\"  Percent correct: {percent_correct:.3f}\")\n",
    "    print(f\"  Estimated information transfer: {percent_correct * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Predictive Modeling of Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTIVE MODELING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Feature engineering for vowel data\n",
    "for condition_data in [vowel_bm, vowel_ci]:\n",
    "    # Add trial number\n",
    "    condition_data['trial_num'] = condition_data.index + 1\n",
    "    # Add normalized trial number\n",
    "    condition_data['trial_norm'] = condition_data['trial_num'] / len(condition_data)\n",
    "    # Add RT z-score\n",
    "    condition_data['rt_zscore'] = (condition_data['rt'] - condition_data['rt'].mean()) / condition_data['rt'].std()\n",
    "\n",
    "# Logistic regression for accuracy prediction\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"\\nLogistic Regression - Predicting Vowel Accuracy:\")\n",
    "for condition_name, condition_data in [('Bimodal', vowel_bm), ('CI', vowel_ci)]:\n",
    "    # Features\n",
    "    features = ['target', 'trial_norm', 'rt_zscore']\n",
    "    X = condition_data[features].fillna(0)\n",
    "    y = condition_data['correct']\n",
    "    \n",
    "    # Model\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    print(f\"\\n{condition_name}:\")\n",
    "    print(f\"  Cross-validated accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    model.fit(X, y)\n",
    "    print(\"  Feature importance (coefficients):\")\n",
    "    for feat, coef in zip(features, model.coef_[0]):\n",
    "        print(f\"    {feat}: {coef:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comprehensive Summary and Clinical Implications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Key findings summary\n",
    "summary = {}\n",
    "\n",
    "# VGRM\n",
    "if 'Overall' in vgrm_results:\n",
    "    summary['VGRM_benefit'] = vgrm_results['Overall']['vgrm']\n",
    "    summary['VGRM_interpretation'] = 'PRESENT' if vgrm_results['Overall']['vgrm'] > 0 else 'ABSENT'\n",
    "\n",
    "# Vowel performance\n",
    "summary['vowel_bm_accuracy'] = vowel_bm['correct'].mean()\n",
    "summary['vowel_ci_accuracy'] = vowel_ci['correct'].mean()\n",
    "summary['vowel_ci_benefit'] = vowel_ci['correct'].mean() - vowel_bm['correct'].mean()\n",
    "\n",
    "# RT\n",
    "summary['rt_bm'] = vowel_bm['rt'].mean()\n",
    "summary['rt_ci'] = vowel_ci['rt'].mean()\n",
    "summary['rt_difference'] = vowel_ci['rt'].mean() - vowel_bm['rt'].mean()\n",
    "\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(f\"1. Voice-Gender Release from Masking (VGRM):\")\n",
    "if 'VGRM_benefit' in summary:\n",
    "    print(f\"   - Overall VGRM: {summary['VGRM_benefit']:.2f} dB\")\n",
    "    print(f\"   - Interpretation: {summary['VGRM_interpretation']}\")\n",
    "    print(f\"   - Clinical implication: {'Patient may benefit from acoustic pitch cues' if summary['VGRM_benefit'] > 0 else 'Limited use of acoustic pitch cues'}\")\n",
    "\n",
    "print(f\"\\n2. Vowel Perception:\")\n",
    "print(f\"   - Bimodal accuracy: {summary['vowel_bm_accuracy']:.3f}\")\n",
    "print(f\"   - CI accuracy: {summary['vowel_ci_accuracy']:.3f}\")\n",
    "print(f\"   - CI benefit/deficit: {summary['vowel_ci_benefit']:.3f}\")\n",
    "print(f\"   - Clinical implication: {'CI provides clearer vowel perception' if summary['vowel_ci_benefit'] > 0 else 'Bimodal provides better vowel perception'}\")\n",
    "\n",
    "print(f\"\\n3. Processing Speed:\")\n",
    "print(f\"   - Bimodal RT: {summary['rt_bm']:.3f} s\")\n",
    "print(f\"   - CI RT: {summary['rt_ci']:.3f} s\")\n",
    "print(f\"   - Difference: {summary['rt_difference']:.3f} s\")\n",
    "print(f\"   - Clinical implication: {'CI requires more processing time' if summary['rt_difference'] > 0 else 'Bimodal requires more processing time'}\")\n",
    "\n",
    "# Generate clinical recommendations\n",
    "print(\"\\nCLINICAL RECOMMENDATIONS:\")\n",
    "recommendations = []\n",
    "\n",
    "if 'VGRM_benefit' in summary and summary['VGRM_benefit'] > 2:\n",
    "    recommendations.append(\"- Strong VGRM benefit suggests good use of acoustic pitch cues\")\n",
    "    recommendations.append(\"- Consider preserving acoustic hearing if possible\")\n",
    "elif 'VGRM_benefit' in summary and summary['VGRM_benefit'] < 0.5:\n",
    "    recommendations.append(\"- Minimal VGRM suggests limited acoustic pitch processing\")\n",
    "    recommendations.append(\"- Bilateral CI may not result in significant pitch-based segregation loss\")\n",
    "\n",
    "if summary['vowel_ci_benefit'] > 0.1:\n",
    "    recommendations.append(\"- CI provides clearer vowel perception than bimodal\")\n",
    "elif summary['vowel_ci_benefit'] < -0.1:\n",
    "    recommendations.append(\"- Bimodal configuration provides better vowel perception\")\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(rec)\n",
    "\n",
    "# Export summary\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv('CI148_comprehensive_summary.csv', index=False)\n",
    "print(\"\\nSummary exported to: CI148_comprehensive_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Additional Exploratory Analyses Suggestions\n",
    "\n",
    "Based on the data, here are additional analyses that could provide valuable insights:\n",
    "\n",
    "### Suggested Analyses:\n",
    "\n",
    "1. **Temporal Dynamics of Fusion**\n",
    "   - Analyze if confusion patterns change over the course of the experiment\n",
    "   - Look for evidence of perceptual learning or fatigue\n",
    "\n",
    "2. **Cross-Modal Correlations**\n",
    "   - Correlate CRM performance with vowel accuracy\n",
    "   - Test if good speech-in-noise ability predicts vowel identification\n",
    "\n",
    "3. **Error Pattern Clustering**\n",
    "   - Use clustering algorithms to identify systematic error patterns\n",
    "   - Group vowels by confusion patterns\n",
    "\n",
    "4. **Asymmetric Confusion Analysis**\n",
    "   - Test if confusion is bidirectional (A→B same as B→A)\n",
    "   - Identify one-way confusion patterns\n",
    "\n",
    "5. **Psychometric Function Fitting**\n",
    "   - Fit sigmoid functions to CRM performance vs SNR\n",
    "   - Extract slope parameters as measure of perceptual precision\n",
    "\n",
    "6. **Sequential Dependencies**\n",
    "   - Analyze if previous trial affects current trial performance\n",
    "   - Look for priming or contrast effects\n",
    "\n",
    "7. **Individual Difference Markers**\n",
    "   - Create composite scores for different abilities\n",
    "   - Develop a perceptual profile\n",
    "\n",
    "8. **Signal Detection Theory Analysis**\n",
    "   - Calculate d' and criterion for each vowel\n",
    "   - Separate sensitivity from response bias\n",
    "\n",
    "9. **Spectral Weighting Analysis**\n",
    "   - Infer which acoustic features drive confusions\n",
    "   - Model feature weights from error patterns\n",
    "\n",
    "10. **Bimodal Integration Efficiency**\n",
    "    - Calculate theoretical optimal integration\n",
    "    - Compare to observed performance\n",
    "\n",
    "Would you like me to implement any of these additional analyses?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
