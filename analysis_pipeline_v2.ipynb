{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reiss Lab Data Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import the necessary libraries for data manipulation, analysis, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Configure plotting style\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Load the raw data files for the vowel, consonant, and CRM experiments. We will also perform initial preprocessing, such as adding column names and mapping identifiers to human-readable labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the directory path containing your data files:  /Users/samipkafle/Downloads/reiss_lab_analysis/data/Data/CI148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for subject: CI148\n",
      "Data directory: /Users/samipkafle/Downloads/reiss_lab_analysis/data/Data/CI148\n",
      "\n",
      "Files found in directory:\n",
      "  .DS_Store\n",
      "  CI148_cons_BM_n_0.out\n",
      "  CI148_consch.all\n",
      "  CI148_crm_0.txt\n",
      "  CI148_crm_1.txt\n",
      "  CI148_crm_10.txt\n",
      "  CI148_crm_2.txt\n",
      "  CI148_crm_3.txt\n",
      "  CI148_crm_4.txt\n",
      "  CI148_crm_5.txt\n",
      "  CI148_crm_6.txt\n",
      "  CI148_crm_7.txt\n",
      "  CI148_crm_8.txt\n",
      "  CI148_crm_9.txt\n",
      "  CI148_vow9_BM_0.txt\n",
      "  CI148_vow9_CI_0.txt\n",
      "  CI148_vowch.all\n",
      "  test_vowch.all\n"
     ]
    }
   ],
   "source": [
    "# Prompt user to input the directory path containing the data files\n",
    "base_path = input(\"Enter the directory path containing your data files: \").strip()\n",
    "\n",
    "# Validate that the path exists\n",
    "if not os.path.isdir(base_path):\n",
    "    raise FileNotFoundError(f\"Directory not found: {base_path}\")\n",
    "\n",
    "# Extract subject ID from the directory name (assumes format like 'CI148')\n",
    "subject_id = os.path.basename(base_path)\n",
    "print(f\"Processing data for subject: {subject_id}\")\n",
    "print(f\"Data directory: {base_path}\")\n",
    "\n",
    "# --- File Paths ---\n",
    "vowel_bm_path = os.path.join(base_path, f'{subject_id}_vow9_BM_0.txt')\n",
    "vowel_ci_path = os.path.join(base_path, f'{subject_id}_vow9_CI_0.txt')\n",
    "consonant_path = os.path.join(base_path, f'{subject_id}_cons_BM_n_0.out')\n",
    "\n",
    "# For CRM, we'll start by loading one file and create a list for scalability\n",
    "crm_files = [os.path.join(base_path, f'{subject_id}_crm_{i}.txt') for i in range(11)]\n",
    "\n",
    "# Show available files in directory\n",
    "print(f\"\\nFiles found in directory:\")\n",
    "for f in sorted(os.listdir(base_path)):\n",
    "    print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vowel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vowel data loaded and preprocessed:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 360 entries, 0 to 359\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   talker_id       360 non-null    int64  \n",
      " 1   vowel_id        360 non-null    int64  \n",
      " 2   response_id     360 non-null    int64  \n",
      " 3   score           360 non-null    int64  \n",
      " 4   rt              360 non-null    float64\n",
      " 5   condition       360 non-null    object \n",
      " 6   vowel_label     360 non-null    object \n",
      " 7   response_label  360 non-null    object \n",
      "dtypes: float64(1), int64(4), object(3)\n",
      "memory usage: 22.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>talker_id</th>\n",
       "      <th>vowel_id</th>\n",
       "      <th>response_id</th>\n",
       "      <th>score</th>\n",
       "      <th>rt</th>\n",
       "      <th>condition</th>\n",
       "      <th>vowel_label</th>\n",
       "      <th>response_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8965</td>\n",
       "      <td>BM</td>\n",
       "      <td>OO</td>\n",
       "      <td>OO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3.5742</td>\n",
       "      <td>BM</td>\n",
       "      <td>OO</td>\n",
       "      <td>OO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7898</td>\n",
       "      <td>BM</td>\n",
       "      <td>AW</td>\n",
       "      <td>AH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.6783</td>\n",
       "      <td>BM</td>\n",
       "      <td>UH</td>\n",
       "      <td>UH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1938</td>\n",
       "      <td>BM</td>\n",
       "      <td>OO</td>\n",
       "      <td>OO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   talker_id  vowel_id  response_id  score      rt condition vowel_label  \\\n",
       "0         16         7            7      1  1.8965        BM          OO   \n",
       "1          8         7            7      1  3.5742        BM          OO   \n",
       "2          9         3            2      0  1.7898        BM          AW   \n",
       "3          4         8            8      1  1.6783        BM          UH   \n",
       "4          4         7            7      1  2.1938        BM          OO   \n",
       "\n",
       "  response_label  \n",
       "0             OO  \n",
       "1             OO  \n",
       "2             AH  \n",
       "3             UH  \n",
       "4             OO  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vowel_cols = ['talker_id', 'vowel_id', 'response_id', 'score', 'rt']\n",
    "\n",
    "# Load individual vowel files\n",
    "df_vowel_bm = pd.read_csv(vowel_bm_path, sep='\\\\s+', header=None, names=vowel_cols)\n",
    "df_vowel_ci = pd.read_csv(vowel_ci_path, sep='\\\\s+', header=None, names=vowel_cols)\n",
    "\n",
    "# Add a 'condition' column to distinguish them\n",
    "df_vowel_bm['condition'] = 'BM' # Bimodal\n",
    "df_vowel_ci['condition'] = 'CI' # Cochlear Implant\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "df_vowel = pd.concat([df_vowel_bm, df_vowel_ci], ignore_index=True)\n",
    "\n",
    "# Define vowel labels from documentation\n",
    "vowel_map = {\n",
    "    1: 'AE', 2: 'AH', 3: 'AW', 4: 'EH', 5: 'IH',\n",
    "    6: 'IY', 7: 'OO', 8: 'UH', 9: 'UW'\n",
    "}\n",
    "\n",
    "# Map IDs to human-readable labels\n",
    "df_vowel['vowel_label'] = df_vowel['vowel_id'].map(vowel_map)\n",
    "df_vowel['response_label'] = df_vowel['response_id'].map(vowel_map)\n",
    "\n",
    "print(\"Vowel data loaded and preprocessed:\")\n",
    "df_vowel.info()\n",
    "df_vowel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Consonant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consonant data loaded and preprocessed:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64 entries, 0 to 63\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   talker_id        64 non-null     int64  \n",
      " 1   consonant_id     64 non-null     int64  \n",
      " 2   response_id      64 non-null     int64  \n",
      " 3   score            64 non-null     int64  \n",
      " 4   rt               64 non-null     float64\n",
      " 5   consonant_label  64 non-null     object \n",
      " 6   response_label   64 non-null     object \n",
      "dtypes: float64(1), int64(4), object(2)\n",
      "memory usage: 3.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>talker_id</th>\n",
       "      <th>consonant_id</th>\n",
       "      <th>response_id</th>\n",
       "      <th>score</th>\n",
       "      <th>rt</th>\n",
       "      <th>consonant_label</th>\n",
       "      <th>response_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20.8202</td>\n",
       "      <td>%</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.4925</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.6630</td>\n",
       "      <td>k</td>\n",
       "      <td>k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8300</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1810</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   talker_id  consonant_id  response_id  score       rt consonant_label  \\\n",
       "0          4            10            1      0  20.8202               %   \n",
       "1          3             2            2      1   3.4925               _   \n",
       "2          4             7            7      1   1.6630               k   \n",
       "3          3             9            9      1   1.8300               n   \n",
       "4          2             8            8      1   2.1810               m   \n",
       "\n",
       "  response_label  \n",
       "0              #  \n",
       "1              _  \n",
       "2              k  \n",
       "3              n  \n",
       "4              m  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consonant_cols = ['talker_id', 'consonant_id', 'response_id', 'score', 'rt']\n",
    "\n",
    "# Load the consonant file\n",
    "df_consonant = pd.read_csv(consonant_path, sep='\\\\s+', header=None, names=consonant_cols)\n",
    "\n",
    "# Define consonant labels from documentation\n",
    "consonant_map = {\n",
    "    1: '#', 2: '_', 3: 'b', 4: 'd', 5: 'f', 6: 'g', 7: 'k',\n",
    "    8: 'm', 9: 'n', 10: '%', 11: 'p', 12: 's', 13: 't',\n",
    "    14: 'v', 15: 'z', 16: '$'\n",
    "}\n",
    "\n",
    "# Map IDs to human-readable labels\n",
    "df_consonant['consonant_label'] = df_consonant['consonant_id'].map(consonant_map)\n",
    "df_consonant['response_label'] = df_consonant['response_id'].map(consonant_map)\n",
    "\n",
    "print(\"Consonant data loaded and preprocessed:\")\n",
    "df_consonant.info()\n",
    "df_consonant.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 CRM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 CRM files:\n",
      "\n",
      "CI148_crm_0.txt:\n",
      "  Talker 0 (M), Maskers 1 & 3 (M/M)\n",
      "  Masker type: same-gender\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 96\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Masker type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmasker_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-gender\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Prompt for condition\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Enter condition for this file (BM/CI/HA): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mupper(ci)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m condition \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHA\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     98\u001b[0m     condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNKNOWN\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1206\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1207\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_crm_header(filepath):\n",
    "    \"\"\"Parse CRM file header to extract talker and masker info.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        header = f.readline()\n",
    "    # Extract: Subject CI148, Talker 0, Maskers 1 and 3, Base atten=15, feedback=0\n",
    "    match = re.search(r'Talker (\\d+), Maskers (\\d+) and (\\d+)', header)\n",
    "    if match:\n",
    "        talker = int(match.group(1))\n",
    "        masker1 = int(match.group(2))\n",
    "        masker2 = int(match.group(3))\n",
    "        return talker, masker1, masker2\n",
    "    return None, None, None\n",
    "\n",
    "def get_gender(talker_id):\n",
    "    \"\"\"Talkers 0-3 are male, 4-7 are female.\"\"\"\n",
    "    return 'M' if talker_id <= 3 else 'F'\n",
    "\n",
    "def get_masker_type(talker, masker1, masker2):\n",
    "    \"\"\"Determine if maskers are same or different gender from target.\"\"\"\n",
    "    target_gender = get_gender(talker)\n",
    "    masker1_gender = get_gender(masker1)\n",
    "    masker2_gender = get_gender(masker2)\n",
    "    if target_gender == masker1_gender == masker2_gender:\n",
    "        return 'same'\n",
    "    elif masker1_gender == masker2_gender and masker1_gender != target_gender:\n",
    "        return 'different'\n",
    "    else:\n",
    "        return 'mixed'\n",
    "\n",
    "def calculate_srt_from_trials(df_run):\n",
    "    \"\"\"\n",
    "    Calculate SRT using adaptive staircase reversal method.\n",
    "    \n",
    "    Assumptions (matching MATLAB code):\n",
    "    - A trial is correct if BOTH color and number are correct\n",
    "    - Track direction changes (correct->incorrect or vice versa)\n",
    "    - Record SNR at each reversal point\n",
    "    - SRT = mean of reversals 5-14 (skipping first 4 for convergence)\n",
    "    - SD = std of reversals 5-14\n",
    "    \"\"\"\n",
    "    snr_values = df_run['snr'].values\n",
    "    color_correct = (df_run['target_color'] == df_run['response_color']).values\n",
    "    number_correct = (df_run['target_number'] == df_run['response_number']).values\n",
    "    correct = color_correct & number_correct\n",
    "    \n",
    "    # Find reversals\n",
    "    reversals = []\n",
    "    prev_correct = correct[0]\n",
    "    \n",
    "    for i in range(1, len(correct)):\n",
    "        if correct[i] != prev_correct:\n",
    "            # Reversal occurred - record the SNR at this trial\n",
    "            reversals.append(snr_values[i])\n",
    "        prev_correct = correct[i]\n",
    "    \n",
    "    # Calculate SRT from reversals 5-14 (indices 4-13, 0-indexed)\n",
    "    if len(reversals) >= 14:\n",
    "        srt_reversals = reversals[4:14]\n",
    "        srt = np.mean(srt_reversals)\n",
    "        sd = np.std(srt_reversals, ddof=0)  # Population std to match MATLAB\n",
    "    elif len(reversals) >= 5:\n",
    "        # Use what we have if fewer than 14 reversals\n",
    "        srt_reversals = reversals[4:]\n",
    "        srt = np.mean(srt_reversals)\n",
    "        sd = np.std(srt_reversals, ddof=0)\n",
    "        print(f\"  Warning: Only {len(reversals)} reversals found (expected 14)\")\n",
    "    else:\n",
    "        srt = np.nan\n",
    "        sd = np.nan\n",
    "        print(f\"  Warning: Insufficient reversals ({len(reversals)}) to calculate SRT\")\n",
    "    \n",
    "    return srt, sd, len(reversals)\n",
    "\n",
    "# Find all CRM files in directory\n",
    "crm_files_found = sorted([f for f in os.listdir(base_path) if '_crm_' in f and f.endswith('.txt')])\n",
    "print(f\"Found {len(crm_files_found)} CRM files:\\n\")\n",
    "\n",
    "crm_cols = ['run', 'target_color', 'response_color', 'target_number', 'response_number', 'snr', 'rt']\n",
    "crm_data_frames = []\n",
    "crm_summary = []\n",
    "\n",
    "for filename in crm_files_found:\n",
    "    filepath = os.path.join(base_path, filename)\n",
    "    \n",
    "    # Parse header for talker/masker info\n",
    "    talker, masker1, masker2 = parse_crm_header(filepath)\n",
    "    masker_type = get_masker_type(talker, masker1, masker2)\n",
    "    \n",
    "    print(f\"{filename}:\")\n",
    "    print(f\"  Talker {talker} ({get_gender(talker)}), Maskers {masker1} & {masker2} ({get_gender(masker1)}/{get_gender(masker2)})\")\n",
    "    print(f\"  Masker type: {masker_type}-gender\")\n",
    "    \n",
    "    # Prompt for condition\n",
    "    condition = input(f\"  Enter condition for this file (BM/CI/HA): \").strip().upper(ci)\n",
    "    if condition not in ['BM', 'CI', 'HA']:\n",
    "        condition = 'UNKNOWN'\n",
    "        print(f\"  Warning: Invalid condition, set to UNKNOWN\")\n",
    "    \n",
    "    # Load trial data\n",
    "    df_temp = pd.read_csv(filepath, sep='\\\\s+', header=None, skiprows=2, names=crm_cols, on_bad_lines='skip')\n",
    "    \n",
    "    # Filter out summary lines (non-numeric in run column)\n",
    "    df_temp = df_temp[pd.to_numeric(df_temp['run'], errors='coerce').notna()].copy()\n",
    "    df_temp = df_temp.astype({'run': int, 'target_color': int, 'response_color': int, \n",
    "                              'target_number': int, 'response_number': int, 'snr': float, 'rt': float})\n",
    "    \n",
    "    # Calculate SRT\n",
    "    srt, sd, n_reversals = calculate_srt_from_trials(df_temp)\n",
    "    print(f\"  Calculated SRT: {srt:.2f} dB, SD: {sd:.2f} dB ({n_reversals} reversals)\")\n",
    "    \n",
    "    # Add metadata to dataframe\n",
    "    df_temp['filename'] = filename\n",
    "    df_temp['condition'] = condition\n",
    "    df_temp['talker'] = talker\n",
    "    df_temp['masker1'] = masker1\n",
    "    df_temp['masker2'] = masker2\n",
    "    df_temp['masker_type'] = masker_type\n",
    "    \n",
    "    crm_data_frames.append(df_temp)\n",
    "    \n",
    "    # Store summary\n",
    "    crm_summary.append({\n",
    "        'filename': filename,\n",
    "        'condition': condition,\n",
    "        'talker': talker,\n",
    "        'talker_gender': get_gender(talker),\n",
    "        'masker1': masker1,\n",
    "        'masker2': masker2,\n",
    "        'masker_type': masker_type,\n",
    "        'n_trials': len(df_temp),\n",
    "        'n_reversals': n_reversals,\n",
    "        'srt': srt,\n",
    "        'sd': sd\n",
    "    })\n",
    "    print()\n",
    "\n",
    "# Combine all CRM data\n",
    "if crm_data_frames:\n",
    "    df_crm = pd.concat(crm_data_frames, ignore_index=True)\n",
    "    df_crm_summary = pd.DataFrame(crm_summary)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Total CRM files loaded: {len(crm_data_frames)}\")\n",
    "    print(f\"Total CRM trials: {len(df_crm)}\")\n",
    "else:\n",
    "    print(\"No CRM files were loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Vowel Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Accuracy\n",
    "vowel_accuracy = df_vowel['score'].mean() * 100\n",
    "print(f\"Overall Vowel Accuracy: {vowel_accuracy:.2f}%\\n\")\n",
    "\n",
    "# Confusion Matrix\n",
    "vowel_labels = [v for k, v in sorted(vowel_map.items())]\n",
    "vowel_cm_counts = pd.crosstab(df_vowel['vowel_label'], df_vowel['response_label'], rownames=['Target'], colnames=['Response'], margins=False, dropna=False).reindex(index=vowel_labels, columns=vowel_labels, fill_value=0)\n",
    "vowel_cm_probs = vowel_cm_counts.div(vowel_cm_counts.sum(axis=1), axis=0).fillna(0)\n",
    "print(\"Vowel Confusion Matrix (Probabilities):\")\n",
    "print(vowel_cm_probs)\n",
    "\n",
    "# Per-Vowel Accuracy\n",
    "per_vowel_accuracy = pd.DataFrame(np.diag(vowel_cm_probs) * 100, index=vowel_cm_probs.index, columns=['Accuracy'])\n",
    "print(\"\\nPer-Vowel Accuracy:\")\n",
    "print(per_vowel_accuracy)\n",
    "\n",
    "# By-Talker Accuracy\n",
    "vowel_talker_accuracy = df_vowel.groupby('talker_id')['score'].mean().reset_index()\n",
    "print(\"\\nBy-Talker Vowel Accuracy:\")\n",
    "print(vowel_talker_accuracy)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(vowel_cm_probs, annot=True, fmt='.2f', cmap='viridis')\n",
    "plt.title('Vowel Confusion Matrix')\n",
    "plt.savefig('vowel_confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=per_vowel_accuracy.index, y='Accuracy', data=per_vowel_accuracy)\n",
    "plt.title('Per-Vowel Accuracy')\n",
    "plt.savefig('per_vowel_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='talker_id', y='score', data=vowel_talker_accuracy)\n",
    "plt.title('By-Talker Vowel Accuracy')\n",
    "plt.savefig('vowel_talker_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_vowel['rt'], bins=20, kde=True)\n",
    "plt.title('Vowel Response Time Distribution')\n",
    "plt.savefig('vowel_rt_histogram.png')\n",
    "plt.show()\n",
    "\n",
    "# Save data\n",
    "vowel_cm_probs.to_csv('vowel_confusion_matrix.csv')\n",
    "per_vowel_accuracy.to_csv('per_vowel_accuracy.csv')\n",
    "vowel_talker_accuracy.to_csv('vowel_talker_accuracy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Consonant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Accuracy\n",
    "consonant_accuracy = df_consonant['score'].mean() * 100\n",
    "print(f\"Overall Consonant Accuracy: {consonant_accuracy:.2f}%\\n\")\n",
    "\n",
    "# Confusion Matrix\n",
    "consonant_labels = [v for k, v in sorted(consonant_map.items())]\n",
    "consonant_cm_counts = pd.crosstab(df_consonant['consonant_label'], df_consonant['response_label'], rownames=['Target'], colnames=['Response'], margins=False, dropna=False).reindex(index=consonant_labels, columns=consonant_labels, fill_value=0)\n",
    "consonant_cm_probs = consonant_cm_counts.div(consonant_cm_counts.sum(axis=1), axis=0).fillna(0)\n",
    "print(\"Consonant Confusion Matrix (Probabilities):\")\n",
    "print(consonant_cm_probs)\n",
    "\n",
    "# Per-Consonant Accuracy\n",
    "per_consonant_accuracy = pd.DataFrame(np.diag(consonant_cm_probs) * 100, index=consonant_cm_probs.index, columns=['Accuracy'])\n",
    "print(\"\\nPer-Consonant Accuracy:\")\n",
    "print(per_consonant_accuracy)\n",
    "\n",
    "# By-Talker Accuracy\n",
    "consonant_talker_accuracy = df_consonant.groupby('talker_id')['score'].mean().reset_index()\n",
    "print(\"\\nBy-Talker Consonant Accuracy:\")\n",
    "print(consonant_talker_accuracy)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(consonant_cm_probs, annot=True, fmt='.2f', cmap='viridis')\n",
    "plt.title('Consonant Confusion Matrix')\n",
    "plt.savefig('consonant_confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=per_consonant_accuracy.index, y='Accuracy', data=per_consonant_accuracy)\n",
    "plt.title('Per-Consonant Accuracy')\n",
    "plt.savefig('per_consonant_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='talker_id', y='score', data=consonant_talker_accuracy)\n",
    "plt.title('By-Talker Consonant Accuracy')\n",
    "plt.savefig('consonant_talker_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_consonant['rt'], bins=20, kde=True)\n",
    "plt.title('Consonant Response Time Distribution')\n",
    "plt.savefig('consonant_rt_histogram.png')\n",
    "plt.show()\n",
    "\n",
    "# Save data\n",
    "consonant_cm_probs.to_csv('consonant_confusion_matrix.csv')\n",
    "per_consonant_accuracy.to_csv('per_consonant_accuracy.csv')\n",
    "consonant_talker_accuracy.to_csv('consonant_talker_accuracy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 CRM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display summary table\n",
    "print(\"CRM Summary Table:\")\n",
    "print(\"=\"*80)\n",
    "display_cols = ['filename', 'condition', 'talker_gender', 'masker_type', 'n_trials', 'srt', 'sd']\n",
    "print(df_crm_summary[display_cols].to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Summary statistics by condition\n",
    "print(\"\\nSRT by Condition:\")\n",
    "print(\"-\"*40)\n",
    "condition_stats = df_crm_summary.groupby('condition').agg({\n",
    "    'srt': ['mean', 'std', 'count'],\n",
    "    'sd': 'mean'\n",
    "}).round(2)\n",
    "condition_stats.columns = ['Mean SRT (dB)', 'SRT SD', 'N runs', 'Mean within-run SD']\n",
    "print(condition_stats)\n",
    "print()\n",
    "\n",
    "# Summary statistics by masker type\n",
    "print(\"\\nSRT by Masker Type:\")\n",
    "print(\"-\"*40)\n",
    "masker_stats = df_crm_summary.groupby('masker_type').agg({\n",
    "    'srt': ['mean', 'std', 'count']\n",
    "}).round(2)\n",
    "masker_stats.columns = ['Mean SRT (dB)', 'SRT SD', 'N runs']\n",
    "print(masker_stats)\n",
    "print()\n",
    "\n",
    "# Summary by condition AND masker type (for VGRM-like analysis)\n",
    "print(\"\\nSRT by Condition × Masker Type:\")\n",
    "print(\"-\"*40)\n",
    "cross_stats = df_crm_summary.groupby(['condition', 'masker_type']).agg({\n",
    "    'srt': ['mean', 'std', 'count']\n",
    "}).round(2)\n",
    "cross_stats.columns = ['Mean SRT (dB)', 'SRT SD', 'N']\n",
    "print(cross_stats)\n",
    "print()\n",
    "\n",
    "# Calculate Voice-Gender Release from Masking (VGRM) if both conditions exist\n",
    "print(\"\\nVoice-Gender Release from Masking (VGRM):\")\n",
    "print(\"-\"*40)\n",
    "print(\"VGRM = SRT(same-gender) - SRT(different-gender)\")\n",
    "print(\"Positive values indicate benefit from different-gender maskers\\n\")\n",
    "\n",
    "for condition in df_crm_summary['condition'].unique():\n",
    "    cond_data = df_crm_summary[df_crm_summary['condition'] == condition]\n",
    "    same_srt = cond_data[cond_data['masker_type'] == 'same']['srt'].mean()\n",
    "    diff_srt = cond_data[cond_data['masker_type'] == 'different']['srt'].mean()\n",
    "    \n",
    "    if not np.isnan(same_srt) and not np.isnan(diff_srt):\n",
    "        vgrm = same_srt - diff_srt\n",
    "        print(f\"{condition}: VGRM = {vgrm:.2f} dB (same: {same_srt:.2f}, diff: {diff_srt:.2f})\")\n",
    "    else:\n",
    "        print(f\"{condition}: Insufficient data for VGRM calculation\")\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: SRT by condition\n",
    "sns.barplot(data=df_crm_summary, x='condition', y='srt', ax=axes[0], errorbar='sd')\n",
    "axes[0].set_xlabel('Condition')\n",
    "axes[0].set_ylabel('SRT (dB)')\n",
    "axes[0].set_title('SRT by Condition')\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 2: SRT by masker type\n",
    "sns.barplot(data=df_crm_summary, x='masker_type', y='srt', ax=axes[1], errorbar='sd')\n",
    "axes[1].set_xlabel('Masker Type')\n",
    "axes[1].set_ylabel('SRT (dB)')\n",
    "axes[1].set_title('SRT by Masker Gender')\n",
    "axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 3: SRT by condition and masker type\n",
    "sns.barplot(data=df_crm_summary, x='condition', y='srt', hue='masker_type', ax=axes[2], errorbar='sd')\n",
    "axes[2].set_xlabel('Condition')\n",
    "axes[2].set_ylabel('SRT (dB)')\n",
    "axes[2].set_title('SRT by Condition × Masker Type')\n",
    "axes[2].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[2].legend(title='Masker Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('crm_srt_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Save summary data\n",
    "df_crm_summary.to_csv('crm_summary.csv', index=False)\n",
    "print(\"\\nSummary saved to crm_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Analysis\n",
    "\n",
    "Additional visualizations and analyses to explore patterns in the data before collecting more subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Trial-by-trial SNR trajectory for each run\n",
    "print(\"Trial-by-trial SNR trajectories (adaptive staircase visualization):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_files = len(df_crm['filename'].unique())\n",
    "n_cols = min(3, n_files)\n",
    "n_rows = int(np.ceil(n_files / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows), squeeze=False)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (filename, group) in enumerate(df_crm.groupby('filename')):\n",
    "    ax = axes[idx]\n",
    "    trials = range(1, len(group) + 1)\n",
    "    ax.plot(trials, group['snr'].values, 'b-o', markersize=3, alpha=0.7)\n",
    "    \n",
    "    # Mark correct vs incorrect trials\n",
    "    correct = (group['target_color'].values == group['response_color'].values) & \\\n",
    "              (group['target_number'].values == group['response_number'].values)\n",
    "    ax.scatter(np.array(trials)[correct], group['snr'].values[correct], \n",
    "               c='green', s=20, zorder=5, label='Correct')\n",
    "    ax.scatter(np.array(trials)[~correct], group['snr'].values[~correct], \n",
    "               c='red', s=20, zorder=5, label='Incorrect')\n",
    "    \n",
    "    # Get SRT for this file\n",
    "    file_srt = df_crm_summary[df_crm_summary['filename'] == filename]['srt'].values[0]\n",
    "    ax.axhline(y=file_srt, color='purple', linestyle='--', alpha=0.7, \n",
    "               label=f'SRT={file_srt:.1f}')\n",
    "    \n",
    "    condition = group['condition'].iloc[0]\n",
    "    masker_type = group['masker_type'].iloc[0]\n",
    "    ax.set_title(f\"{filename}\\n{condition}, {masker_type}-gender\", fontsize=9)\n",
    "    ax.set_xlabel('Trial')\n",
    "    ax.set_ylabel('SNR (dB)')\n",
    "    ax.axhline(y=0, color='gray', linestyle=':', alpha=0.5)\n",
    "    if idx == 0:\n",
    "        ax.legend(fontsize=7, loc='upper right')\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_files, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('crm_staircase_trajectories.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Response time analysis\n",
    "print(\"Response Time Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# RT distribution overall\n",
    "sns.histplot(df_crm['rt'], bins=30, kde=True, ax=axes[0])\n",
    "axes[0].set_xlabel('Response Time (s)')\n",
    "axes[0].set_title('Overall RT Distribution')\n",
    "axes[0].axvline(x=df_crm['rt'].median(), color='r', linestyle='--', \n",
    "                label=f'Median: {df_crm[\"rt\"].median():.2f}s')\n",
    "axes[0].legend()\n",
    "\n",
    "# RT by condition\n",
    "sns.boxplot(data=df_crm, x='condition', y='rt', ax=axes[1])\n",
    "axes[1].set_xlabel('Condition')\n",
    "axes[1].set_ylabel('Response Time (s)')\n",
    "axes[1].set_title('RT by Condition')\n",
    "\n",
    "# RT vs SNR (does RT increase at harder SNRs?)\n",
    "axes[2].scatter(df_crm['snr'], df_crm['rt'], alpha=0.3, s=10)\n",
    "# Add trend line\n",
    "z = np.polyfit(df_crm['snr'], df_crm['rt'], 1)\n",
    "p = np.poly1d(z)\n",
    "snr_range = np.linspace(df_crm['snr'].min(), df_crm['snr'].max(), 100)\n",
    "axes[2].plot(snr_range, p(snr_range), 'r-', alpha=0.8, label=f'Trend')\n",
    "axes[2].set_xlabel('SNR (dB)')\n",
    "axes[2].set_ylabel('Response Time (s)')\n",
    "axes[2].set_title('RT vs SNR')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('crm_response_time_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# RT statistics\n",
    "print(\"\\nRT Statistics by Condition:\")\n",
    "rt_stats = df_crm.groupby('condition')['rt'].agg(['mean', 'median', 'std']).round(2)\n",
    "rt_stats.columns = ['Mean RT (s)', 'Median RT (s)', 'RT SD (s)']\n",
    "print(rt_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Error pattern analysis\n",
    "print(\"Error Pattern Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate error types\n",
    "df_crm['color_correct'] = df_crm['target_color'] == df_crm['response_color']\n",
    "df_crm['number_correct'] = df_crm['target_number'] == df_crm['response_number']\n",
    "df_crm['both_correct'] = df_crm['color_correct'] & df_crm['number_correct']\n",
    "\n",
    "# Error breakdown\n",
    "def categorize_error(row):\n",
    "    if row['both_correct']:\n",
    "        return 'Correct'\n",
    "    elif row['color_correct'] and not row['number_correct']:\n",
    "        return 'Number Error'\n",
    "    elif not row['color_correct'] and row['number_correct']:\n",
    "        return 'Color Error'\n",
    "    else:\n",
    "        return 'Both Error'\n",
    "\n",
    "df_crm['error_type'] = df_crm.apply(categorize_error, axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Overall error breakdown\n",
    "error_counts = df_crm['error_type'].value_counts()\n",
    "colors = {'Correct': 'green', 'Number Error': 'orange', 'Color Error': 'blue', 'Both Error': 'red'}\n",
    "error_counts.plot(kind='bar', ax=axes[0], color=[colors.get(x, 'gray') for x in error_counts.index])\n",
    "axes[0].set_xlabel('Error Type')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Overall Error Breakdown')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Error rates by condition\n",
    "error_by_condition = pd.crosstab(df_crm['condition'], df_crm['error_type'], normalize='index') * 100\n",
    "error_by_condition.plot(kind='bar', ax=axes[1], color=[colors.get(x, 'gray') for x in error_by_condition.columns])\n",
    "axes[1].set_xlabel('Condition')\n",
    "axes[1].set_ylabel('Percentage')\n",
    "axes[1].set_title('Error Types by Condition (%)')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "axes[1].legend(title='Error Type', bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('crm_error_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nError rates by condition:\")\n",
    "print(error_by_condition.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Individual run comparison plot\n",
    "print(\"Individual Run Comparison:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sort by SRT for visualization\n",
    "df_sorted = df_crm_summary.sort_values('srt')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create color map for conditions\n",
    "condition_colors = {'BM': 'blue', 'CI': 'red', 'HA': 'green', 'UNKNOWN': 'gray'}\n",
    "colors = [condition_colors.get(c, 'gray') for c in df_sorted['condition']]\n",
    "\n",
    "# Create hatching for masker type\n",
    "bars = ax.bar(range(len(df_sorted)), df_sorted['srt'], color=colors, \n",
    "              edgecolor='black', linewidth=1)\n",
    "\n",
    "# Add error bars for SD\n",
    "ax.errorbar(range(len(df_sorted)), df_sorted['srt'], yerr=df_sorted['sd'],\n",
    "            fmt='none', color='black', capsize=3)\n",
    "\n",
    "# Add hatching for different-gender maskers\n",
    "for idx, (bar, masker_type) in enumerate(zip(bars, df_sorted['masker_type'])):\n",
    "    if masker_type == 'different':\n",
    "        bar.set_hatch('//')\n",
    "\n",
    "ax.set_xticks(range(len(df_sorted)))\n",
    "ax.set_xticklabels([f.replace(subject_id + '_crm_', '').replace('.txt', '') \n",
    "                    for f in df_sorted['filename']], rotation=45)\n",
    "ax.set_xlabel('Run Number')\n",
    "ax.set_ylabel('SRT (dB)')\n",
    "ax.set_title(f'SRT by Run for {subject_id}\\n(Hatched = different-gender maskers)')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=condition_colors[c], label=c) \n",
    "                   for c in df_sorted['condition'].unique()]\n",
    "legend_elements.append(Patch(facecolor='white', edgecolor='black', hatch='//', label='Diff-gender'))\n",
    "ax.legend(handles=legend_elements, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('crm_individual_runs.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4.5 Summary statistics and data quality checks\n",
    "print(\"Data Quality Summary:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nSubject: {subject_id}\")\n",
    "print(f\"Total CRM runs: {len(df_crm_summary)}\")\n",
    "print(f\"Total trials: {len(df_crm)}\")\n",
    "print(f\"\\nConditions tested: {', '.join(df_crm_summary['condition'].unique())}\")\n",
    "print(f\"Masker types: {', '.join(df_crm_summary['masker_type'].unique())}\")\n",
    "\n",
    "print(\"\\nRuns per condition:\")\n",
    "print(df_crm_summary['condition'].value_counts())\n",
    "\n",
    "print(\"\\nRuns per masker type:\")\n",
    "print(df_crm_summary['masker_type'].value_counts())\n",
    "\n",
    "print(\"\\nReversal count check (should be 14 for valid runs):\")\n",
    "rev_check = df_crm_summary[['filename', 'n_reversals', 'srt']]\n",
    "print(rev_check.to_string(index=False))\n",
    "\n",
    "# Flag any runs with fewer than 14 reversals\n",
    "low_rev = df_crm_summary[df_crm_summary['n_reversals'] < 14]\n",
    "if len(low_rev) > 0:\n",
    "    print(\"\\n⚠️  Warning: The following runs have fewer than 14 reversals:\")\n",
    "    print(low_rev[['filename', 'n_reversals']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OVERALL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMean SRT across all runs: {df_crm_summary['srt'].mean():.2f} ± {df_crm_summary['srt'].std():.2f} dB\")\n",
    "print(f\"SRT range: {df_crm_summary['srt'].min():.2f} to {df_crm_summary['srt'].max():.2f} dB\")\n",
    "print(f\"\\nMean accuracy: {df_crm['both_correct'].mean()*100:.1f}%\")\n",
    "print(f\"Median response time: {df_crm['rt'].median():.2f} s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
