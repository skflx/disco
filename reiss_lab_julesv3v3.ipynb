{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reiss Lab Analysis Pipeline v3.0\n",
    "\n",
    "**Subject-Level Analysis for Vowel, Consonant, and CRM Experiments**\n",
    "\n",
    "This notebook provides a comprehensive, production-quality analysis pipeline for processing pilot data from Reiss Lab experiments. It merges and enhances previous analysis versions by focusing on data density, statistical rigor, and reproducibility.\n",
    "\n",
    "**Key Features:**\n",
    "- **Configuration-Driven:** CRM conditions are assigned via a hard-coded dictionary, eliminating interactive prompts and ensuring reproducibility.\n",
    "- **Data-Dense Visualizations:** All plots are static and designed to be information-rich, suitable for publication.\n",
    "- **Statistical Rigor:** All reported metrics include sample sizes (n) and error margins (95% CI or SEM), and statistical tests (ANOVA/t-tests) provide full output.\n",
    "- **Comprehensive Exploratory Analysis:** Includes phonetic feature analysis, reaction time analysis, talker-specific performance, granular CRM masking effects, and session-level temporal analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "This section imports all necessary libraries and defines the configuration for the analysis. Key configurations, such as the CRM condition mapping and phonetic feature definitions, are hard-coded here to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Configure plotting style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [12, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "\n",
    "# CRM Condition Mapping: Assign CRM file indices to conditions.\n",
    "# The user should edit this dictionary based on their experimental log.\n",
    "# Example: 'BM': [0, 1, 2, 8] means _crm_0.txt, _crm_1.txt, etc., are Bimodal.\n",
    "CRM_CONDITION_MAP = {\n",
    "    'BM': [1, 2, 3, 8],\n",
    "    'CI': [4, 6, 10],\n",
    "    'HA': [5, 7, 9]\n",
    "}\n",
    "\n",
    "# Phonetic Feature Map for Consonants\n",
    "# (Voicing, Place, Manner)\n",
    "FEATURE_MAP = {\n",
    "    'b': (1, 'Bilabial', 'Plosive'),\n",
    "    'd': (1, 'Alveolar', 'Plosive'),\n",
    "    'g': (1, 'Velar',    'Plosive'),\n",
    "    'p': (0, 'Bilabial', 'Plosive'),\n",
    "    't': (0, 'Alveolar', 'Plosive'),\n",
    "    'k': (0, 'Velar',    'Plosive'),\n",
    "    'm': (1, 'Bilabial', 'Nasal'),\n",
    "    'n': (1, 'Alveolar', 'Nasal'),\n",
    "    'f': (0, 'Labiodental', 'Fricative'),\n",
    "    'v': (1, 'Labiodental', 'Fricative'),\n",
    "    's': (0, 'Alveolar',    'Fricative'),\n",
    "    'z': (1, 'Alveolar',    'Fricative'),\n",
    "    '#': (0, 'Palato-Alveolar', 'Fricative'), # 'sh'\n",
    "    '_': (1, 'Palato-Alveolar', 'Fricative'), # 'zh'\n",
    "    '%': (0, 'Palato-Alveolar', 'Affricate'), # 'ch'\n",
    "    '$': (1, 'Palato-Alveolar', 'Affricate')  # 'j'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "This section defines functions to load and preprocess data for each experiment type (Vowel, Consonant, CRM). The main `load_subject_data` function orchestrates this process, taking a subject ID and data path as input and returning a dictionary of DataFrames. The CRM loading process automatically assigns conditions based on the `CRM_CONDITION_MAP` defined in the configuration section, ensuring a reproducible workflow without manual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def _load_vowel_data(base_path, subject_id):\n",
    "    vowel_cols = ['talker_id', 'vowel_id', 'response_id', 'score', 'rt']\n",
    "    vowel_map = {1: 'AE', 2: 'AH', 3: 'AW', 4: 'EH', 5: 'IH', 6: 'IY', 7: 'OO', 8: 'UH', 9: 'UW'}\n",
    "    dfs = []\n",
    "    for cond in ['BM', 'CI']:\n",
    "        fpath = os.path.join(base_path, f'{subject_id}_vow9_{cond}_0.txt')\n",
    "        if os.path.exists(fpath):\n",
    "            df = pd.read_csv(fpath, sep='\\\\s+', header=None, names=vowel_cols)\n",
    "            df['condition'] = cond\n",
    "            dfs.append(df)\n",
    "    if not dfs:\n",
    "        return None\n",
    "    df_vowel = pd.concat(dfs, ignore_index=True)\n",
    "    df_vowel['vowel_label'] = df_vowel['vowel_id'].map(vowel_map)\n",
    "    df_vowel['response_label'] = df_vowel['response_id'].map(vowel_map)\n",
    "    return df_vowel\n",
    "\n",
    "def _load_consonant_data(base_path, subject_id):\n",
    "    cons_cols = ['talker_id', 'consonant_id', 'response_id', 'score', 'rt']\n",
    "    cons_map = {1: '#', 2: '_', 3: 'b', 4: 'd', 5: 'f', 6: 'g', 7: 'k', 8: 'm', 9: 'n', 10: '%', 11: 'p', 12: 's', 13: 't', 14: 'v', 15: 'z', 16: '$'}\n",
    "    fpath = os.path.join(base_path, f'{subject_id}_cons_BM_n_0.out')\n",
    "    if not os.path.exists(fpath):\n",
    "        return None\n",
    "    df_consonant = pd.read_csv(fpath, sep='\\\\s+', header=None, names=cons_cols)\n",
    "    df_consonant['consonant_label'] = df_consonant['consonant_id'].map(cons_map)\n",
    "    df_consonant['response_label'] = df_consonant['response_id'].map(cons_map)\n",
    "    return df_consonant\n",
    "\n",
    "def _load_crm_data(base_path, subject_id, condition_map):\n",
    "    crm_cols = ['run', 'target_color', 'response_color', 'target_number', 'response_number', 'snr', 'rt']\n",
    "    crm_files = sorted([f for f in os.listdir(base_path) if '_crm_' in f and f.endswith('.txt')])\n",
    "    \n",
    "    if not crm_files:\n",
    "        return None, None\n",
    "\n",
    "    # Invert the condition map for easy lookup\n",
    "    file_idx_to_condition = {idx: cond for cond, indices in condition_map.items() for idx in indices}\n",
    "\n",
    "    crm_data, crm_summary_list = [], []\n",
    "    for i, f in enumerate(crm_files):\n",
    "        fpath = os.path.join(base_path, f)\n",
    "        talker, m1, m2 = parse_crm_header(fpath)\n",
    "        masker_type = get_masker_type(talker, m1, m2)\n",
    "        condition = file_idx_to_condition.get(i, 'Unknown')\n",
    "\n",
    "        try:\n",
    "            df_temp = pd.read_csv(fpath, sep='\\\\s+', header=None, skiprows=2, names=crm_cols, on_bad_lines='skip')\n",
    "            df_temp = df_temp[pd.to_numeric(df_temp['run'], errors='coerce').notna()].astype(float)\n",
    "            srt, sd, revs = calculate_srt(df_temp)\n",
    "\n",
    "            df_temp['filename'] = f\n",
    "            df_temp['condition'] = condition\n",
    "            df_temp['masker_type'] = masker_type\n",
    "            df_temp['talker_gender'] = get_gender(talker)\n",
    "            df_temp['masker_gender'] = get_gender(m1) # Assuming both maskers are same gender\n",
    "            crm_data.append(df_temp)\n",
    "\n",
    "            crm_summary_list.append({\n",
    "                'filename': f, 'condition': condition, 'masker_type': masker_type,\n",
    "                'srt': srt, 'sd': sd, 'reversals': revs, \n",
    "                'talker_gender': get_gender(talker)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {f}: {e}\")\n",
    "\n",
    "    df_crm = pd.concat(crm_data, ignore_index=True)\n",
    "    df_crm_summary = pd.DataFrame(crm_summary_list)\n",
    "    return df_crm, df_crm_summary\n",
    "\n",
    "# --- Main Data Loading Orchestrator ---\n",
    "def load_subject_data(subject_id, data_path, crm_map):\n",
    "    base_path = os.path.join(data_path, subject_id)\n",
    "    if not os.path.isdir(base_path):\n",
    "        print(f\"Directory not found: {base_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f'--- Loading Data for Subject: {subject_id} ---')\n",
    "    data = {}\n",
    "    data['vowel'] = _load_vowel_data(base_path, subject_id)\n",
    "    data['consonant'] = _load_consonant_data(base_path, subject_id)\n",
    "    data['crm'], data['crm_summary'] = _load_crm_data(base_path, subject_id, crm_map)\n",
    "    \n",
    "    print('--- Load Summary ---')\n",
    "    for name, df in data.items():\n",
    "        status = f'{len(df)} records loaded' if df is not None else 'Not found'\n",
    "        print(f'{name.capitalize():<12}: {status}')\n",
    "    print('--------------------\\n')\n",
    "    return data\n",
    "\n",
    "# --- Utility functions needed by CRM loader ---\n",
    "def parse_crm_header(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            header = f.readline()\n",
    "        match = re.search(r'Talker (\\\\d+), Maskers (\\\\d+) and (\\\\d+)', header)\n",
    "        if match:\n",
    "            return int(match.group(1)), int(match.group(2)), int(match.group(3))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None, None, None\n",
    "\n",
    "def get_gender(talker_id):\n",
    "    if talker_id is None: return 'U'\n",
    "    return 'M' if talker_id <= 3 else 'F'\n",
    "\n",
    "def get_masker_type(talker, masker1, masker2):\n",
    "    if talker is None: return 'unknown'\n",
    "    t_gen, m1_gen, m2_gen = get_gender(talker), get_gender(masker1), get_gender(masker2)\n",
    "    return 'same' if t_gen == m1_gen == m2_gen else 'different' if m1_gen == m2_gen else 'mixed'\n",
    "\n",
    "def calculate_srt(df_run):\n",
    "    if df_run.empty: return np.nan, np.nan, 0\n",
    "    snr = df_run['snr'].values\n",
    "    correct = ((df_run['target_color'] == df_run['response_color']) & \n",
    "               (df_run['target_number'] == df_run['response_number'])).values\n",
    "    reversals = []\n",
    "    if len(correct) < 2: return np.nan, np.nan, 0\n",
    "    prev = correct[0]\n",
    "    for i in range(1, len(correct)):\n",
    "        if correct[i] != prev:\n",
    "            reversals.append(snr[i])\n",
    "        prev = correct[i]\n",
    "    \n",
    "    if len(reversals) >= 5:\n",
    "        calc_revs = reversals[4:14] if len(reversals) >= 14 else reversals[4:]\n",
    "        return np.mean(calc_revs), np.std(calc_revs, ddof=1), len(reversals)\n",
    "    return np.nan, np.nan, len(reversals)\n",
    "\n",
    "# --- Execute Data Loading ---\n",
    "SUBJECT_ID = 'CI148'  # CHANGE THIS TO YOUR SUBJECT ID\n",
    "DATA_PATH = '/app/data/Data' # CHANGE THIS TO THE PARENT 'Data' DIRECTORY\n",
    "\n",
    "try:\n",
    "    # This call will load all data for the specified subject\n",
    "    subject_data = load_subject_data(SUBJECT_ID, DATA_PATH, CRM_CONDITION_MAP)\n",
    "    # For convenience, unpack the dataframes into their own variables\n",
    "    df_vowel = subject_data.get('vowel')\n",
    "    df_consonant = subject_data.get('consonant')\n",
    "    df_crm = subject_data.get('crm')\n",
    "    df_crm_summary = subject_data.get('crm_summary')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred during data loading: {e}')\n",
    "    print('Please ensure SUBJECT_ID and DATA_PATH are set correctly.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vowel and Consonant Analysis\n",
    "\n",
    "This section analyzes phoneme identification performance, including overall accuracy, confusion matrices, phonetic feature transmission for consonants, reaction times, and performance by talker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.1 Vowel Analysis ---\n",
    "\n",
    "if df_vowel is not None:\n",
    "    print('--- Vowel Identification Analysis ---')\n",
    "    # Overall Accuracy\n",
    "    vowel_accuracy = df_vowel['score'].agg(['mean', 'sem', 'count'])\n",
    "    print(f\"\\nOverall Accuracy: {vowel_accuracy['mean']*100:.2f}% (SEM={vowel_accuracy['sem']*100:.2f}, n={vowel_accuracy['count']})\\n\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    vowel_labels = [v for k, v in sorted({1: 'AE', 2: 'AH', 3: 'AW', 4: 'EH', 5: 'IH', 6: 'IY', 7: 'OO', 8: 'UH', 9: 'UW'}.items())]\n",
    "    cm = pd.crosstab(df_vowel['vowel_label'], df_vowel['response_label'], normalize='index').reindex(index=vowel_labels, columns=vowel_labels, fill_value=0)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f', cmap='viridis', ax=ax[0])\n",
    "    ax[0].set_title('Vowel Confusion Matrix (Probability)')\n",
    "    \n",
    "    # Accuracy per Vowel\n",
    "    per_vowel_stats = df_vowel.groupby('vowel_label')['score'].agg(['mean', 'sem', 'count']).reindex(vowel_labels)\n",
    "    per_vowel_stats['mean'] *= 100\n",
    "    per_vowel_stats['sem'] *= 100\n",
    "    sns.barplot(x=per_vowel_stats.index, y=per_vowel_stats['mean'], ax=ax[1], palette='viridis')\n",
    "    ax[1].errorbar(x=per_vowel_stats.index, y=per_vowel_stats['mean'], yerr=per_vowel_stats['sem'] * 1.96, fmt='none', c='black', capsize=5)\n",
    "    ax[1].set_title(f'Per-Vowel Accuracy (n={len(df_vowel)})')\n",
    "    ax[1].set_ylabel('% Correct (with 95% CI)')\n",
    "    ax[1].set_xlabel('Vowel')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Vowel data not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.2 Consonant Phonetic Feature Analysis ---\n",
    "if df_consonant is not None:\n",
    "    print('\\n--- Consonant Feature Transmission Analysis ---')\n",
    "    \n",
    "    def get_features(label):\n",
    "        return FEATURE_MAP.get(label, (np.nan, np.nan, np.nan))\n",
    "\n",
    "    df_cons_features = df_consonant.copy()\n",
    "    df_cons_features[['target_voicing', 'target_place', 'target_manner']] = df_cons_features['consonant_label'].apply(get_features).apply(pd.Series)\n",
    "    df_cons_features[['response_voicing', 'response_place', 'response_manner']] = df_cons_features['response_label'].apply(get_features).apply(pd.Series)\n",
    "    \n",
    "    features = ['voicing', 'place', 'manner']\n",
    "    feature_accuracy = {}\n",
    "    for feature in features:\n",
    "        correct = (df_cons_features[f'target_{feature}'] == df_cons_features[f'response_{feature}']).sum()\n",
    "        total = df_cons_features[f'target_{feature}'].notna().sum()\n",
    "        feature_accuracy[feature.capitalize()] = (correct / total) * 100 if total > 0 else 0\n",
    "\n",
    "    feature_df = pd.Series(feature_accuracy).reset_index()\n",
    "    feature_df.columns = ['Feature', 'Accuracy']\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x='Feature', y='Accuracy', data=feature_df, palette='colorblind')\n",
    "    plt.title(f'Consonant Feature Transmission (n={len(df_consonant)})')\n",
    "    plt.ylabel('% Correct')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Consonant data not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.3 Reaction Time and Talker Performance Analysis ---\n",
    "\n",
    "def plot_rt_and_talker(df, experiment_name):\n",
    "    if df is None or df.empty:\n",
    "        print(f'{experiment_name} data not available for this analysis.')\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    # Reaction Time Distribution\n",
    "    sns.histplot(df['rt'], kde=True, ax=ax[0])\n",
    "    mean_rt, std_rt = df['rt'].mean(), df['rt'].std()\n",
    "    ax[0].axvline(mean_rt, color='r', linestyle='--', label=f'Mean: {mean_rt:.2f}s')\n",
    "    ax[0].set_title(f'{experiment_name} Reaction Time (n={len(df)})\\nMean={mean_rt:.2f}s, Std={std_rt:.2f}s')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Talker Performance (Accuracy)\n",
    "    talker_stats = df.groupby('talker_id')['score'].agg(['mean', 'sem', 'count'])\n",
    "    talker_stats['mean'] *= 100\n",
    "    talker_stats['sem'] *= 100\n",
    "    sns.barplot(x=talker_stats.index, y=talker_stats['mean'], ax=ax[1], palette='coolwarm')\n",
    "    ax[1].errorbar(x=range(len(talker_stats)), y=talker_stats['mean'], yerr=talker_stats['sem'] * 1.96, fmt='none', c='black', capsize=5)\n",
    "    ax[1].set_title(f'{experiment_name} Accuracy by Talker')\n",
    "    ax[1].set_ylabel('% Correct (with 95% CI)')\n",
    "    ax[1].set_xlabel('Talker ID')\n",
    "    \n",
    "    # Add talker gender for context if possible\n",
    "    talker_genders = {tid: get_gender(tid) for tid in talker_stats.index}\n",
    "    ax[1].set_xticklabels([f\"{tid} ({talker_genders[tid]})\" for tid in talker_stats.index])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print('\\n--- Reaction Time & Talker Performance ---')\n",
    "plot_rt_and_talker(df_vowel, 'Vowel')\n",
    "plot_rt_and_talker(df_consonant, 'Consonant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive CRM Analysis\n",
    "\n",
    "This section provides a detailed analysis of the Coordinate Response Measure (CRM) task, focusing on Speech Reception Thresholds (SRTs) across various conditions. It includes statistical tests to determine the significance of these differences and granular analysis of error patterns and reaction times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.1 SRT Breakdown and Statistical Testing ---\n",
    "\n",
    "if df_crm_summary is not None:\n",
    "    print('--- CRM Speech Reception Threshold (SRT) Analysis ---')\n",
    "    \n",
    "    # Filter out runs with no valid SRT\n",
    "    df_srt = df_crm_summary.dropna(subset=['srt'])\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "    # SRT by Condition\n",
    "    sns.barplot(data=df_srt, x='condition', y='srt', ax=ax[0], errorbar=('ci', 95), capsize=.1)\n",
    "    ax[0].set_title(f'SRT by Listening Condition (n={len(df_srt)})')\n",
    "    ax[0].set_ylabel('SRT (dB SNR) with 95% CI')\n",
    "\n",
    "    # SRT by Masker Type\n",
    "    sns.barplot(data=df_srt, x='masker_type', y='srt', ax=ax[1], errorbar=('ci', 95), capsize=.1)\n",
    "    ax[1].set_title(f'SRT by Masker Type (n={len(df_srt)})')\n",
    "    ax[1].set_ylabel('')\n",
    "\n",
    "    # SRT by Interaction\n",
    "    sns.pointplot(data=df_srt, x='condition', y='srt', hue='masker_type', ax=ax[2], dodge=True, capsize=.1)\n",
    "    ax[2].set_title('Interaction between Condition and Masker Type')\n",
    "    ax[2].set_ylabel('')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Statistical Analysis (ANOVA) ---\n",
    "    print('\\n--- Two-Way ANOVA: SRT ~ Condition + Masker Type ---')\n",
    "    model = ols('srt ~ C(condition) * C(masker_type)', data=df_srt).fit()\n",
    "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "    print(anova_table)\n",
    "\n",
    "    # --- T-Tests for specific comparisons ---\n",
    "    print('\\n--- Post-hoc T-Tests for Masker Type within each Condition ---')\n",
    "    for condition in df_srt['condition'].unique():\n",
    "        cond_df = df_srt[df_srt['condition'] == condition]\n",
    "        same_gender = cond_df[cond_df['masker_type'] == 'same']['srt']\n",
    "        diff_gender = cond_df[cond_df['masker_type'] == 'different']['srt']\n",
    "        if len(same_gender) > 1 and len(diff_gender) > 1:\n",
    "            ttest_res = stats.ttest_ind(same_gender, diff_gender)\n",
    "            print(f\"Condition '{condition}': t-statistic={ttest_res.statistic:.2f}, p-value={ttest_res.pvalue:.3f}\")\n",
    "        else:\n",
    "            print(f\"Condition '{condition}': Not enough data for t-test.\")\n",
    "else:\n",
    "    print('CRM data not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.2 Granular Talker-Masker Gender Analysis ---\n",
    "if df_crm_summary is not None:\n",
    "    print('\\n--- Granular Analysis of Talker-Masker Gender Effects ---')\n",
    "    df_srt = df_crm_summary.dropna(subset=['srt']).copy()\n",
    "    \n",
    "    # Create a specific interaction term for plotting\n",
    "    df_srt['gender_combo'] = df_srt['talker_gender'] + ' talker / ' + df_srt['masker_type'] + ' maskers'\n",
    "    \n",
    "    # F-MM vs M-FF comparison\n",
    "    f_mm_srt = df_srt[(df_srt['talker_gender'] == 'F') & (df_srt['masker_type'] == 'different')]['srt']\n",
    "    m_ff_srt = df_srt[(df_srt['talker_gender'] == 'M') & (df_srt['masker_type'] == 'different')]['srt']\n",
    "\n",
    "    if not f_mm_srt.empty and not m_ff_srt.empty:\n",
    "        f_mm_mean, f_mm_sem = f_mm_srt.mean(), f_mm_srt.sem()\n",
    "        m_ff_mean, m_ff_sem = m_ff_srt.mean(), m_ff_srt.sem()\n",
    "        print(f\"\\nFemale Talker / Male Maskers (F-MM): SRT = {f_mm_mean:.2f} (SEM={f_mm_sem:.2f}, n={len(f_mm_srt)})\")\n",
    "        print(f\"Male Talker / Female Maskers (M-FF): SRT = {m_ff_mean:.2f} (SEM={m_ff_sem:.2f}, n={len(m_ff_srt)})\")\n",
    "        \n",
    "        if len(f_mm_srt) > 1 and len(m_ff_srt) > 1:\n",
    "            ttest_res_gender = stats.ttest_ind(f_mm_srt, m_ff_srt)\n",
    "            print(f\"T-test (F-MM vs M-FF): t-statistic={ttest_res_gender.statistic:.2f}, p-value={ttest_res_gender.pvalue:.3f}\")\n",
    "    \n",
    "    # Plotting all gender combos\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=df_srt, x='gender_combo', y='srt', errorbar=('ci', 95), capsize=.1, palette='muted')\n",
    "    plt.title(f'SRT by Talker-Masker Gender Combination (n={len(df_srt)})')\n",
    "    plt.ylabel('SRT (dB SNR) with 95% CI')\n",
    "    plt.xlabel('Gender Combination')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('CRM data not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.3 CRM Error and Reaction Time Analysis ---\n",
    "\n",
    "if df_crm is not None:\n",
    "    print('\\n--- CRM Error Type & Reaction Time Analysis ---')\n",
    "    # Calculate correctness for each component\n",
    "    df_crm['color_correct'] = df_crm['target_color'] == df_crm['response_color']\n",
    "    df_crm['number_correct'] = df_crm['target_number'] == df_crm['response_number']\n",
    "    df_crm['overall_correct'] = df_crm['color_correct'] & df_crm['number_correct']\n",
    "\n",
    "    def get_error_type(row):\n",
    "        if row['overall_correct']: return 'Correct'\n",
    "        if not row['color_correct'] and row['number_correct']: return 'Color Error'\n",
    "        if row['color_correct'] and not row['number_correct']: return 'Number Error'\n",
    "        return 'Both Error'\n",
    "    \n",
    "    df_crm['error_type'] = df_crm.apply(get_error_type, axis=1)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    # Error type breakdown\n",
    "    error_dist = df_crm['error_type'].value_counts(normalize=True).mul(100)\n",
    "    sns.barplot(x=error_dist.index, y=error_dist.values, ax=ax[0], palette='pastel')\n",
    "    ax[0].set_title(f'CRM Error Type Distribution (n={len(df_crm)})')\n",
    "    ax[0].set_ylabel('% of Trials')\n",
    "\n",
    "    # RT vs SNR\n",
    "    sns.scatterplot(data=df_crm, x='snr', y='rt', hue='overall_correct', ax=ax[1], alpha=0.5)\n",
    "    ax[1].set_title('Reaction Time vs. SNR')\n",
    "    ax[1].set_xlabel('SNR (dB)')\n",
    "    ax[1].set_ylabel('Reaction Time (s)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('CRM data not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.4 Adaptive Track Visualization ---\n",
    "\n",
    "if df_crm is not None:\n",
    "    print('\\n--- Visualization of Adaptive Staircase Tracks ---')\n",
    "    filenames = sorted(df_crm['filename'].unique())\n",
    "    n_files = len(filenames)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_files + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, filename in enumerate(filenames):\n",
    "        run_data = df_crm[df_crm['filename'] == filename]\n",
    "        run_summary = df_crm_summary[df_crm_summary['filename'] == filename].iloc[0]\n",
    "        \n",
    "        ax = axes[i]\n",
    "        sns.lineplot(x=range(len(run_data)), y=run_data['snr'], marker='o', ax=ax, label='SNR Track')\n",
    "        ax.axhline(run_summary['srt'], ls='--', color='red', label=f\"SRT: {run_summary['srt']:.2f} dB\")\n",
    "        ax.set_title(f\"{filename} ({run_summary['condition']}, {run_summary['masker_type']})\", fontsize=10)\n",
    "        ax.set_ylabel('SNR (dB)')\n",
    "        ax.set_xlabel('Trial Number')\n",
    "        ax.legend()\n",
    "\n",
    "    for i in range(n_files, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('CRM data not available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Experiment and Session-Level Analysis\n",
    "\n",
    "This final section examines relationships between different tasks and looks for performance trends across the entire experimental session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1 Correlation Analysis: Phoneme vs. Sentence Performance ---\n",
    "\n",
    "if df_vowel is not None and df_crm_summary is not None:\n",
    "    print('--- Correlation Analysis ---')\n",
    "    \n",
    "    # Get overall scores\n",
    "    overall_vowel_acc = df_vowel['score'].mean() * 100\n",
    "    overall_cons_acc = df_consonant['score'].mean() * 100 if df_consonant is not None else np.nan\n",
    "    avg_srt = df_crm_summary['srt'].mean()\n",
    "\n",
    "    # Create a summary for this subject\n",
    "    correlation_data = {\n",
    "        'Vowel Accuracy (%)': overall_vowel_acc,\n",
    "        'Consonant Accuracy (%)': overall_cons_acc,\n",
    "        'Average SRT (dB)': avg_srt\n",
    "    }\n",
    "    \n",
    "    # Note: Correlation is not meaningful for a single subject, but this structure\n",
    "    # can be extended to a multi-subject dataframe.\n",
    "    print(\"Performance Summary for this Subject:\")\n",
    "    for key, value in correlation_data.items():\n",
    "        print(f'- {key}: {value:.2f}')\n",
    "        \n",
    "    # Placeholder for future multi-subject regression plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax[0].scatter(overall_vowel_acc, avg_srt, s=100, label=SUBJECT_ID)\n",
    "    ax[0].set_title('Vowel Accuracy vs. Average SRT (Example)')\n",
    "    ax[0].set_xlabel('Vowel Accuracy (%)')\n",
    "    ax[0].set_ylabel('Average SRT (dB)')\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    ax[1].scatter(overall_cons_acc, avg_srt, s=100, label=SUBJECT_ID)\n",
    "    ax[1].set_title('Consonant Accuracy vs. Average SRT (Example)')\n",
    "    ax[1].set_xlabel('Consonant Accuracy (%)')\n",
    "    ax[1].set_ylabel('Average SRT (dB)')\n",
    "    ax[1].grid(True)\n",
    "    \n",
    "    fig.suptitle('Example Cross-Task Correlation (for multi-subject analysis)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Data for correlation analysis not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.2 Temporal Analysis (Fatigue/Learning) ---\n",
    "if df_vowel is not None and df_crm is not None:\n",
    "    print('\\n--- Session-Level Temporal Analysis ---')\n",
    "\n",
    "    # Combine all trials into a single timeline, assuming file order is chronological\n",
    "    # This is a simplified approach; a more robust way would use timestamps if available.\n",
    "    df_vowel['task'] = 'Vowel'\n",
    "    df_consonant['task'] = 'Consonant' if df_consonant is not None else ''\n",
    "    df_crm['task'] = 'CRM'\n",
    "    df_crm['score'] = (df_crm['target_color'] == df_crm['response_color']) & (df_crm['target_number'] == df_crm['response_number'])\n",
    "    \n",
    "    # Create a global trial index\n",
    "    df_vowel['global_trial'] = range(len(df_vowel))\n",
    "    if df_consonant is not None:\n",
    "        df_consonant['global_trial'] = range(len(df_vowel), len(df_vowel) + len(df_consonant))\n",
    "    df_crm['global_trial'] = range(len(df_vowel) + (len(df_consonant) if df_consonant is not None else 0), len(df_vowel) + (len(df_consonant) if df_consonant is not None else 0) + len(df_crm))\n",
    "    \n",
    "    all_trials = pd.concat([df_vowel[['global_trial', 'score', 'rt', 'task']], \n",
    "                            df_consonant[['global_trial', 'score', 'rt', 'task']] if df_consonant is not None else pd.DataFrame(),\n",
    "                            df_crm[['global_trial', 'score', 'rt', 'task']]], ignore_index=True)\n",
    "    \n",
    "    # Calculate rolling accuracy\n",
    "    all_trials['rolling_acc'] = all_trials['score'].rolling(window=50, min_periods=10).mean() * 100\n",
    "    all_trials['rolling_rt'] = all_trials['rt'].rolling(window=50, min_periods=10).mean()\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(15, 6))\n",
    "    \n",
    "    # Plot rolling accuracy\n",
    "    sns.lineplot(data=all_trials, x='global_trial', y='rolling_acc', color='blue', ax=ax1, label='Rolling Accuracy (50 trials)')\n",
    "    ax1.set_xlabel('Global Trial Number (Chronological)')\n",
    "    ax1.set_ylabel('Rolling Accuracy (%)', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.set_ylim(0, 100)\n",
    "    \n",
    "    # Create a second y-axis for rolling RT\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.lineplot(data=all_trials, x='global_trial', y='rolling_rt', color='red', ax=ax2, label='Rolling RT (50 trials)')\n",
    "    ax2.set_ylabel('Rolling Reaction Time (s)', color='red')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    \n",
    "    # Add vertical lines to delineate tasks\n",
    "    if df_consonant is not None:\n",
    "        ax1.axvline(x=len(df_vowel), color='k', linestyle='--', alpha=0.5, label='Task Change')\n",
    "    ax1.axvline(x=len(df_vowel) + (len(df_consonant) if df_consonant is not None else 0), color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.title('Session-Level Performance Trends (Learning/Fatigue)')\n",
    "    fig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print('Data for temporal analysis not available.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}